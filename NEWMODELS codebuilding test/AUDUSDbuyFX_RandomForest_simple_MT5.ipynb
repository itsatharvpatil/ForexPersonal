{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b814e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Candle: 0.007993959062752477\n"
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "from talipp.indicators import EMA, SMA, Stoch, DPO\n",
    "from joblib import dump\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, confusion_matrix, classification_report, recall_score, accuracy_score, f1_score, roc_auc_score\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from own_functions import label_data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hardcoded credentials - as requested\n",
    "login = 51708234\n",
    "password = \"4bM&wuVJcBTnjV\"\n",
    "server = \"ICMarketsEU-Demo\"\n",
    "\n",
    "mt.initialize()\n",
    "mt.login(login, password, server)\n",
    "\n",
    "symbol = \"AUDUSD\"\n",
    "timeframe = mt.TIMEFRAME_D1\n",
    "start_date = datetime(2010, 1, 1)\n",
    "end_date = datetime(2024, 11, 10)\n",
    "StopLoss = 1\n",
    "TakeProfit = 1\n",
    "BreakEvenRatio = StopLoss / (StopLoss + TakeProfit)\n",
    "\n",
    "def add_rolling_features(df, window):\n",
    "    df['rolling_mean_open'] = df['open'].rolling(window=window).mean()\n",
    "    df['rolling_std_open'] = df['open'].rolling(window=window).std()\n",
    "    df['rolling_mean_close'] = df['close'].rolling(window=window).mean()\n",
    "    df['rolling_std_close'] = df['close'].rolling(window=window).std()\n",
    "    df['rolling_mean_high'] = df['high'].rolling(window=window).mean()\n",
    "    df['rolling_std_high'] = df['high'].rolling(window=window).std()\n",
    "    df['rolling_mean_low'] = df['low'].rolling(window=window).mean()\n",
    "    df['rolling_std_low'] = df['low'].rolling(window=window).std()\n",
    "    return df\n",
    "\n",
    "def add_lag_features(df, lags):\n",
    "    for lag in lags:\n",
    "        df[f'open_lag_{lag}'] = df['open'].shift(lag)\n",
    "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "        df[f'high_lag_{lag}'] = df['high'].shift(lag)\n",
    "        df[f'low_lag_{lag}'] = df['low'].shift(lag)\n",
    "    return df\n",
    "\n",
    "def extract_rolling_features(df, signal, symbol, max_shift=20, min_shift=5):\n",
    "    df_melted = df[['time', signal]].copy()\n",
    "    df_melted[\"Symbols\"] = symbol\n",
    "    df_rolled = roll_time_series(df_melted, column_id=\"Symbols\", column_sort=\"time\",\n",
    "                                 max_timeshift=max_shift, min_timeshift=min_shift)\n",
    "    X = extract_features(df_rolled.drop(\"Symbols\", axis=1),\n",
    "                         column_id=\"id\", column_sort=\"time\", column_value=signal,\n",
    "                         impute_function=impute, show_warnings=False)\n",
    "    X = X.set_index(X.index.map(lambda x: x[1]), drop=True)\n",
    "    X.index.name = \"time\"\n",
    "    return X.dropna()\n",
    "\n",
    "# Fetch historical data\n",
    "ohlc_data = pd.DataFrame(mt.copy_rates_range(symbol, timeframe, start_date, end_date))\n",
    "ohlc_data['time'] = pd.to_datetime(ohlc_data['time'], unit='s')\n",
    "df = ohlc_data[['time', 'open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "df['EMA_9'] = talib.EMA(df['close'], timeperiod=9)\n",
    "df['EMA_21'] = talib.EMA(df['close'], timeperiod=21)\n",
    "df['EMA_50'] = talib.EMA(df['close'], timeperiod=50)\n",
    "\n",
    "df['RSI_9'] = talib.RSI(df['close'], timeperiod=9)\n",
    "df['RSI_14'] = talib.RSI(df['close'], timeperiod=14)\n",
    "df['RSI_21'] = talib.RSI(df['close'], timeperiod=21)\n",
    "\n",
    "df['WILLR_15'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=15)\n",
    "df['WILLR_23'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=23)\n",
    "df['WILLR_42'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=42)\n",
    "df['WILLR_145'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=145)\n",
    "\n",
    "df['SAR'] = talib.SAR(df['high'], df['low'], acceleration=0.02, maximum=0.2)\n",
    "\n",
    "df['BB_upper'], df['BB_middle'], df['BB_lower'] = talib.BBANDS(df['close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "df['BB_width'] = df['BB_upper'] - df['BB_lower']\n",
    "\n",
    "df['MACD'], df['MACD_signal'], df['MACD_hist'] = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "df['CCI_14'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "\n",
    "df = add_rolling_features(df, window=5)\n",
    "df = add_lag_features(df, lags=[1, 2, 3, 4, 5])\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df['b_flag'] = 0\n",
    "df['s_flag'] = 0\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "#csv_file_path = 'EURUSD_D1_2010to101124.csv'  # Specify your desired path\n",
    "#df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "label_data(df, [StopLoss], [TakeProfit], 80, symbol, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324e96de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 1s:\n",
      "b_flag: 1850\n",
      "s_flag: 1756\n",
      "\n",
      "Counts in segments of 100% data:\n",
      "b_flag: 1850\n",
      "s_flag: 1756\n",
      "\n",
      "Counts in intervals of 10%:\n",
      "0% - 10%: b_flag=186, s_flag=177\n",
      "10% - 20%: b_flag=176, s_flag=194\n",
      "20% - 30%: b_flag=192, s_flag=176\n",
      "30% - 40%: b_flag=178, s_flag=190\n",
      "40% - 50%: b_flag=212, s_flag=156\n",
      "50% - 60%: b_flag=165, s_flag=206\n",
      "60% - 70%: b_flag=196, s_flag=171\n",
      "70% - 80%: b_flag=181, s_flag=190\n",
      "80% - 90%: b_flag=194, s_flag=174\n",
      "90% - 100%: b_flag=170, s_flag=122\n",
      "100% - 110%: b_flag=0, s_flag=0\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of 1s in b_flag and s_flag columns\n",
    "total_b_flags = df['b_flag'].sum()\n",
    "total_s_flags = df['s_flag'].sum()\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate counts in segments of complete 100% data\n",
    "count_100_b_flags = total_b_flags\n",
    "count_100_s_flags = total_s_flags\n",
    "\n",
    "# Calculate counts in intervals of 10%\n",
    "interval_counts = []\n",
    "for i in range(0, 101, 10):\n",
    "    start_idx = int(i / 100 * total_rows)\n",
    "    end_idx = int((i + 10) / 100 * total_rows)\n",
    "    \n",
    "    interval_b_flags = df['b_flag'].iloc[start_idx:end_idx].sum()\n",
    "    interval_s_flags = df['s_flag'].iloc[start_idx:end_idx].sum()\n",
    "    \n",
    "    interval_counts.append((f'{i}% - {i+10}%', interval_b_flags, interval_s_flags))\n",
    "\n",
    "# Print results\n",
    "print(\"Total number of 1s:\")\n",
    "print(f\"b_flag: {total_b_flags}\")\n",
    "print(f\"s_flag: {total_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in segments of 100% data:\")\n",
    "print(f\"b_flag: {count_100_b_flags}\")\n",
    "print(f\"s_flag: {count_100_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in intervals of 10%:\")\n",
    "for interval, count_b, count_s in interval_counts:\n",
    "    print(f\"{interval}: b_flag={count_b}, s_flag={count_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bdd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 40/40 [00:08<00:00,  4.98it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [01:29<00:00,  2.24s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:07<00:00,  5.14it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [01:22<00:00,  2.06s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:07<00:00,  5.10it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [01:18<00:00,  1.95s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:07<00:00,  5.57it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [01:11<00:00,  1.78s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:05<00:00,  7.99it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:54<00:00,  1.36s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.31it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.31it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:52<00:00,  1.32s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  8.40it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:59<00:00,  1.49s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  8.10it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [01:01<00:00,  1.54s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  8.13it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:54<00:00,  1.37s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.05it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:56<00:00,  1.42s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.12it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:55<00:00,  1.39s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.55it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:54<00:00,  1.36s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.63it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:54<00:00,  1.37s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.58it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['s_flag'], inplace=True)\n",
    "\n",
    "X1 = extract_rolling_features(df, 'WILLR_15', symbol)\n",
    "X2 = extract_rolling_features(df, 'WILLR_42', symbol)\n",
    "X3 = extract_rolling_features(df, 'RSI_14', symbol)\n",
    "X4 = extract_rolling_features(df, 'MACD_hist', symbol)\n",
    "X5 = extract_rolling_features(df, 'EMA_9', symbol)\n",
    "X6 = extract_rolling_features(df, 'EMA_21', symbol)\n",
    "X7 = extract_rolling_features(df, 'EMA_50', symbol)\n",
    "X9 = extract_rolling_features(df, 'RSI_9', symbol)\n",
    "X10 = extract_rolling_features(df, 'RSI_21', symbol)\n",
    "X11 = extract_rolling_features(df, 'WILLR_23', symbol)\n",
    "X12 = extract_rolling_features(df, 'WILLR_145', symbol)\n",
    "X13 = extract_rolling_features(df, 'SAR', symbol)\n",
    "X14 = extract_rolling_features(df, 'BB_width', symbol)\n",
    "X15 = extract_rolling_features(df, 'MACD_signal', symbol)\n",
    "X16 = extract_rolling_features(df, 'CCI_14', symbol)\n",
    "\n",
    "# Combine all extracted features\n",
    "X = pd.concat([X1, X2, X3, X4, X5, X6, X7, X9, X10, X11, X12, X13, X14, X15, X16], axis=1, join='inner').dropna()\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.set_index('time')\n",
    "X = X[X.index.isin(df.index)]\n",
    "X = pd.concat([df,X], axis=1, join='inner')\n",
    "\n",
    "X_df = select_features(X, X['b_flag'])\n",
    "X_df = X_df[[col for col in X_df if col != 'b_flag'] + ['b_flag']]\n",
    "\n",
    "correlation_matrix = X_df.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "high_correlation_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "X_df = X_df.drop(columns=high_correlation_features)\n",
    "\n",
    "original_index = X_df.index\n",
    "#X_df = X_df.shift(periods=1, axis=0)\n",
    "X_df.index = original_index\n",
    "X_df = X_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de92c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before PCA: 357\n",
      "Number of features after PCA: 15\n",
      "Confusion Matrix:\n",
      "[[175  27]\n",
      " [102  67]]\n",
      "Accuracy: 0.6523\n",
      "Precision: 0.7128\n",
      "Recall: 0.3964\n",
      "F1 Score: 0.5095\n",
      "ROC-AUC: 0.7031\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.87      0.73       202\n",
      "           1       0.71      0.40      0.51       169\n",
      "\n",
      "    accuracy                           0.65       371\n",
      "   macro avg       0.67      0.63      0.62       371\n",
      "weighted avg       0.67      0.65      0.63       371\n",
      "\n",
      "WIN/LOSS-Diff: 21.28 %\n",
      "False Positives: 27\n",
      "True Positives: 67\n",
      "Ratio total: 71.28\n",
      "BreakEvenRatio: 0.5\n",
      "____________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "split = int(0.90 * len(X_df))\n",
    "train_data, test_data = X_df.iloc[:split], X_df.iloc[split:]\n",
    "\n",
    "x_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data['b_flag'].values\n",
    "x_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data['b_flag'].values\n",
    "\n",
    "sc_mt = StandardScaler()\n",
    "x_train = sc_mt.fit_transform(x_train)\n",
    "x_test = sc_mt.transform(x_test)\n",
    "\n",
    "print(\"Number of features before PCA:\", x_train.shape[1])\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "\n",
    "pca = PCA(n_components=15, svd_solver='randomized', random_state=0)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "print(\"Number of features after PCA:\", x_train.shape[1])\n",
    "\n",
    "n_estimators = 150\n",
    "class_weight = {0: 1, 1: 5}\n",
    "max_features = 'sqrt'\n",
    "random_state = 0\n",
    "\n",
    "rf_classifier_mt = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    class_weight=class_weight,\n",
    "    max_features=max_features,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "rf_classifier_mt.fit(x_train, y_train)\n",
    "y_pred_proba = rf_classifier_mt.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Add probability threshold for predicting class 1\n",
    "threshold = 0.6  # Adjust as needed to increase precision\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "false_positives = conf_matrix[0][1]\n",
    "true_positives = conf_matrix[1][1]\n",
    "\n",
    "precision = precision_score(y_test, y_pred) if (false_positives+true_positives) > 0 else 0\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print('Accuracy:', round(accuracy, 4))\n",
    "print('Precision:', round(precision, 4))\n",
    "print('Recall:', round(recall, 4))\n",
    "print('F1 Score:', round(f1, 4))\n",
    "print('ROC-AUC:', round(roc_auc, 4))\n",
    "print('Classification Report:\\n', classification_rep)\n",
    "print('WIN/LOSS-Diff:', round(100 * (precision - BreakEvenRatio), 2), '%')\n",
    "print('False Positives:', false_positives)\n",
    "print('True Positives:', true_positives)\n",
    "if (false_positives + true_positives) > 0:\n",
    "    print('Ratio total:', round(100 * (true_positives / (false_positives + true_positives)), 2))\n",
    "print('BreakEvenRatio:', round(BreakEvenRatio, 2))\n",
    "print('____________________________________________________________________________________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36887e71",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AUDUSD_D1_3112buy/feature_names.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m X_df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAUDUSD_D1_3112buy/feature_names.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mlist\u001b[39m(feature_names), f)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\forex_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AUDUSD_D1_3112buy/feature_names.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "feature_names = X_df.columns\n",
    "with open('AUDUSD_D1_3112buy/feature_names.json', 'w') as f:\n",
    "    json.dump(list(feature_names), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f65221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert index to datetime without 'unit' since the format is already date strings\n",
    "X_df.index = pd.to_datetime(X_df.index)\n",
    "\n",
    "# Format datetime to the desired string format\n",
    "X_df.index = X_df.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Creating a DataFrame for predictions with the correct index\n",
    "df_pred = pd.DataFrame(index=X_df.iloc[split:].index)  # No need for split+1\n",
    "df_pred['prediction'] = y_pred\n",
    "\n",
    "# Save to CSV\n",
    "df_pred.to_csv('predAUDUSD_D1_3112buy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece23c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
