{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbb1f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\forex_env\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Candle: 0.01323269578065998\n"
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import talib\n",
    "from talipp.indicators import EMA, SMA, Stoch, DPO\n",
    "from joblib import dump\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, confusion_matrix, classification_report\n",
    "from own_functions import *\n",
    "import os\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, make_forecasting_frame\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "mt.initialize()\n",
    "login = 51708234\n",
    "password =\"4bM&wuVJcBTnjV\"\n",
    "server = \"ICMarketsEU-Demo\"\n",
    "mt.login(login,password,server)\n",
    "\n",
    "symbol = \"GBPUSD\"\n",
    "timeframe = mt.TIMEFRAME_D1\n",
    "ohlc_data = pd.DataFrame(mt.copy_rates_range(symbol, timeframe, datetime(2000, 1, 1), datetime(2023, 12, 31)))\n",
    "ohlc_data['time'] = pd.to_datetime(ohlc_data['time'], unit='s')\n",
    "df = ohlc_data[['time', 'open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "\n",
    "def add_rolling_features(df, window):\n",
    "    df['rolling_mean_open'] = df['open'].rolling(window=window).mean()\n",
    "    df['rolling_std_open'] = df['open'].rolling(window=window).std()\n",
    "    df['rolling_mean_close'] = df['close'].rolling(window=window).mean()\n",
    "    df['rolling_std_close'] = df['close'].rolling(window=window).std()\n",
    "    df['rolling_mean_high'] = df['high'].rolling(window=window).mean()\n",
    "    df['rolling_std_high'] = df['high'].rolling(window=window).std()\n",
    "    df['rolling_mean_low'] = df['low'].rolling(window=window).mean()\n",
    "    df['rolling_std_low'] = df['low'].rolling(window=window).std()\n",
    "    return df\n",
    "\n",
    "# Function to add lag features\n",
    "def add_lag_features(df, lags):\n",
    "    for lag in lags:\n",
    "        df[f'open_lag_{lag}'] = df['open'].shift(lag)\n",
    "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "        df[f'high_lag_{lag}'] = df['high'].shift(lag)\n",
    "        df[f'low_lag_{lag}'] = df['low'].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Indicators\n",
    "# Calculate indicators\n",
    "df['WILLR_15'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=15)\n",
    "df['WILLR_23'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=23)\n",
    "df['WILLR_42'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=42)\n",
    "df['WILLR_145'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=145)\n",
    "\n",
    "df = add_rolling_features(df, window=5)\n",
    "df = add_lag_features(df, lags=[1, 2, 3, 4, 5])\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Buy & Sell Flags\n",
    "df['b_flag'] = 0\n",
    "df['s_flag'] = 0\n",
    "\n",
    "# Dropping NaN values and resetting index\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\"\"\" csv_file_path = 'ICMT5EURUSD2000_043024_D1.csv'  # Specify your desired path\n",
    "df.to_csv(csv_file_path, index=False) \"\"\"\n",
    "\n",
    "StopLoss = 1\n",
    "TakeProfit = 1\n",
    "BreakEvenRatio=StopLoss/(StopLoss+TakeProfit)\n",
    "label_data(df,[StopLoss],[TakeProfit],80,symbol,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36f2e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 1s:\n",
      "b_flag: 2985\n",
      "s_flag: 2973\n",
      "\n",
      "Counts in segments of 100% data:\n",
      "b_flag: 2985\n",
      "s_flag: 2973\n",
      "\n",
      "Counts in intervals of 10%:\n",
      "0% - 10%: b_flag=307, s_flag=301\n",
      "10% - 20%: b_flag=350, s_flag=256\n",
      "20% - 30%: b_flag=307, s_flag=298\n",
      "30% - 40%: b_flag=282, s_flag=296\n",
      "40% - 50%: b_flag=289, s_flag=317\n",
      "50% - 60%: b_flag=315, s_flag=292\n",
      "60% - 70%: b_flag=269, s_flag=336\n",
      "70% - 80%: b_flag=322, s_flag=286\n",
      "80% - 90%: b_flag=322, s_flag=286\n",
      "90% - 100%: b_flag=222, s_flag=305\n",
      "100% - 110%: b_flag=0, s_flag=0\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of 1s in b_flag and s_flag columns\n",
    "total_b_flags = df['b_flag'].sum()\n",
    "total_s_flags = df['s_flag'].sum()\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate counts in segments of complete 100% data\n",
    "count_100_b_flags = total_b_flags\n",
    "count_100_s_flags = total_s_flags\n",
    "\n",
    "# Calculate counts in intervals of 10%\n",
    "interval_counts = []\n",
    "for i in range(0, 101, 10):\n",
    "    start_idx = int(i / 100 * total_rows)\n",
    "    end_idx = int((i + 10) / 100 * total_rows)\n",
    "    \n",
    "    interval_b_flags = df['b_flag'].iloc[start_idx:end_idx].sum()\n",
    "    interval_s_flags = df['s_flag'].iloc[start_idx:end_idx].sum()\n",
    "    \n",
    "    interval_counts.append((f'{i}% - {i+10}%', interval_b_flags, interval_s_flags))\n",
    "\n",
    "# Print results\n",
    "print(\"Total number of 1s:\")\n",
    "print(f\"b_flag: {total_b_flags}\")\n",
    "print(f\"s_flag: {total_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in segments of 100% data:\")\n",
    "print(f\"b_flag: {count_100_b_flags}\")\n",
    "print(f\"s_flag: {count_100_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in intervals of 10%:\")\n",
    "for interval, count_b, count_s in interval_counts:\n",
    "    print(f\"{interval}: b_flag={count_b}, s_flag={count_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af671f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  8.65it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [01:15<00:00,  1.90s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  8.19it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [01:20<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WILLR_42__mean_second_derivative_central', 'WILLR_15', 'WILLR_15__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"max\"', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_10', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_7', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_9', 'WILLR_42__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"min\"', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_8', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_6', 'WILLR_15__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"max\"', 'WILLR_42__agg_linear_trend__attr_\"rvalue\"__chunk_len_10__f_agg_\"mean\"', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_5', 'WILLR_15__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"min\"', 'WILLR_145', 'WILLR_42__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"max\"', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_7', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_7', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_8', 'WILLR_15__index_mass_quantile__q_0.9', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_6', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_8', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_8', 'WILLR_42__energy_ratio_by_chunks__num_segments_10__segment_focus_9', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_9', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_6', 'WILLR_15__agg_linear_trend__attr_\"stderr\"__chunk_len_5__f_agg_\"max\"', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_4', 'WILLR_42__index_mass_quantile__q_0.9', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_5', 'WILLR_15__agg_linear_trend__attr_\"stderr\"__chunk_len_5__f_agg_\"min\"', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.2', 'WILLR_42__agg_linear_trend__attr_\"stderr\"__chunk_len_5__f_agg_\"min\"', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_5', 'WILLR_15__number_peaks__n_1', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_1', 'WILLR_42__fft_coefficient__attr_\"real\"__coeff_3', 'WILLR_42__number_peaks__n_1', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.0', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_7', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.4', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_9', 'WILLR_42__agg_linear_trend__attr_\"stderr\"__chunk_len_5__f_agg_\"max\"', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.0', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.4__ql_0.0', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_9', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.4__ql_0.0', 'WILLR_42__energy_ratio_by_chunks__num_segments_10__segment_focus_5', 'WILLR_15__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"max\"', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_4', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_4', 'WILLR_42__has_duplicate_min', 'WILLR_42__number_cwt_peaks__n_1', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.8', 'WILLR_42__binned_entropy__max_bins_10', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_3', 'WILLR_15__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_2', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.2__ql_0.0', 'WILLR_15__energy_ratio_by_chunks__num_segments_10__segment_focus_4', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_4', 'WILLR_15__energy_ratio_by_chunks__num_segments_10__segment_focus_6', 'WILLR_42__cwt_coefficients__coeff_12__w_2__widths_(2, 5, 10, 20)', 'WILLR_15__number_cwt_peaks__n_1', 'WILLR_15__has_duplicate_min', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.6', 'WILLR_15__longest_strike_above_mean', 'WILLR_42__approximate_entropy__m_2__r_0.1', 'WILLR_42__cwt_coefficients__coeff_3__w_5__widths_(2, 5, 10, 20)', 'rolling_std_low', 'WILLR_15__first_location_of_maximum', 'WILLR_15__symmetry_looking__r_0.2', 'WILLR_15__minimum', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.6', 'WILLR_42__percentage_of_reoccurring_values_to_all_values', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_10', 'WILLR_15__percentage_of_reoccurring_datapoints_to_all_datapoints', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_3', 'WILLR_15__cwt_coefficients__coeff_1__w_5__widths_(2, 5, 10, 20)', 'WILLR_42__absolute_maximum', 'WILLR_42__cwt_coefficients__coeff_5__w_2__widths_(2, 5, 10, 20)', 'WILLR_42__cwt_coefficients__coeff_4__w_2__widths_(2, 5, 10, 20)', 'WILLR_15__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.6', 'WILLR_42__value_count__value_0', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.4', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.8', 'b_flag']\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "df.drop(columns=['s_flag'], inplace=True)\n",
    "\n",
    "selected_signal_1 = 'WILLR_15'\n",
    "df_melted_1 = df[['time', selected_signal_1]].copy()\n",
    "df_melted_1[\"Symbols\"] = symbol\n",
    "\n",
    "df_rolled_1 = roll_time_series(df_melted_1, column_id=\"Symbols\", column_sort=\"time\",\n",
    "                               max_timeshift=20, min_timeshift=5)\n",
    "\n",
    "X1 = extract_features(df_rolled_1.drop(\"Symbols\", axis=1), \n",
    "                      column_id=\"id\", column_sort=\"time\", column_value=selected_signal_1, \n",
    "                      impute_function=impute, show_warnings=False)\n",
    "\n",
    "X1 = X1.set_index(X1.index.map(lambda x: x[1]), drop=True)\n",
    "X1.index.name = \"time\"\n",
    "X1 = X1.dropna()\n",
    "\n",
    "selected_signal_2 = 'WILLR_42'\n",
    "df_melted_2 = df[['time', selected_signal_2]].copy()\n",
    "df_melted_2[\"Symbols\"] = symbol\n",
    "\n",
    "df_rolled_2 = roll_time_series(df_melted_2, column_id=\"Symbols\", column_sort=\"time\",\n",
    "                               max_timeshift=20, min_timeshift=5)\n",
    "\n",
    "X2 = extract_features(df_rolled_2.drop(\"Symbols\", axis=1), \n",
    "                      column_id=\"id\", column_sort=\"time\", column_value=selected_signal_2, \n",
    "                      impute_function=impute, show_warnings=False)\n",
    "\n",
    "X2 = X2.set_index(X2.index.map(lambda x: x[1]), drop=True)\n",
    "X2.index.name = \"time\"\n",
    "X2 = X2.dropna()\n",
    "\n",
    "X = pd.concat([X1, X2], axis=1, join='inner')\n",
    "X = X.dropna()\n",
    "\n",
    "# Align indices\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.set_index('time')\n",
    "df = df[df.index.isin(X.index)]\n",
    "\n",
    "X = pd.concat([X, df], axis=1, join='inner')\n",
    "\n",
    "# Ensure b_flag is at the end after feature selection\n",
    "X_df = select_features(X, X['b_flag'])\n",
    "X_df = X_df[[col for col in X_df if col != 'b_flag'] + ['b_flag']]\n",
    "\n",
    "correlation_matrix = X_df.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "high_correlation_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.8)]\n",
    "X_df = X_df.drop(columns=high_correlation_features)\n",
    "\n",
    "\n",
    "original_index = X_df.index\n",
    "shifted_X_df = X_df.shift(periods=1,axis=0)\n",
    "shifted_X_df.index = original_index\n",
    "\n",
    "X_df = shifted_X_df.dropna()\n",
    "\n",
    "\n",
    "# Get the list of selected feature names\n",
    "selected_feature_names_X = list(X_df.columns)\n",
    "\n",
    "# Combine lists if you need a single list for all selected features\n",
    "\n",
    "print(selected_feature_names_X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c5c425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "The best model was from fold 1 with an F1 score of 0.7352\n",
      "Best threshold for this model: 0.50\n",
      "Model saved to GBPUSD_D1_3112_NEWBuy/best_model_fold_1.joblib\n",
      "Overall Training Data Metrics:\n",
      "Precision: 0.7786304412783016\n",
      "Recall: 0.8638897078851212\n",
      "F1 Score: 0.8189964758506472\n",
      "Overall Testing Data Metrics:\n",
      "Precision: 0.6451314612669227\n",
      "Recall: 0.7634328943071673\n",
      "F1 Score: 0.6981876456146756\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "# Function to adjust threshold for better prediction\n",
    "def adjust_threshold(probas, target):\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    for threshold in np.arange(0.1, 1.0, 0.1):\n",
    "        preds = (probas >= threshold).astype(int)\n",
    "        f1 = f1_score(target, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold\n",
    "\n",
    "# Setup directories\n",
    "os.makedirs('GBPUSD_D1_3112_NEWBuy', exist_ok=True)\n",
    "\n",
    "# TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define models for ensemble\n",
    "clf1 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    random_state=0,\n",
    "    max_leaf_nodes=10,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    ccp_alpha=0.01,\n",
    "    class_weight={0: 10, 1: 15}\n",
    ")\n",
    "\n",
    "clf2 = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "clf3 = LogisticRegression(random_state=0)\n",
    "\n",
    "# Ensemble model - Voting Classifier\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('rf', clf1),\n",
    "    ('gb', clf2),\n",
    "    ('lr', clf3)\n",
    "], voting='soft')  # Use 'soft' voting for better probability estimates\n",
    "\n",
    "# Ensure the time series order is preserved in the split\n",
    "X = X_df.iloc[:, :-1].values\n",
    "y = X_df['b_flag'].values\n",
    "\n",
    "# Initialize variables for aggregating results and tracking the best model\n",
    "overall_precision_sum_train = 0\n",
    "overall_recall_sum_train = 0\n",
    "overall_f1_sum_train = 0\n",
    "overall_precision_sum_test = 0\n",
    "overall_recall_sum_test = 0\n",
    "overall_f1_sum_test = 0\n",
    "best_f1_score = -np.inf\n",
    "best_model_fold = None\n",
    "best_model = None\n",
    "best_test_predictions = None\n",
    "best_test_thresholded_predictions = None\n",
    "best_test_indices = None\n",
    "best_threshold_value = None  # To store the best threshold\n",
    "split_num = 0  # Initialize split_num here\n",
    "\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_df)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Train data\n",
    "    x_train, y_train = X[train_index], y[train_index]\n",
    "    x_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "    # Scale Data\n",
    "    sc_mt = StandardScaler()\n",
    "    x_train = sc_mt.fit_transform(x_train)\n",
    "    x_test = sc_mt.transform(x_test)\n",
    "\n",
    "    # Save scaler\n",
    "    dump(sc_mt, f'GBPUSD_D1_3112_NEWBuy/scaler_fold_{fold + 1}.joblib')\n",
    "\n",
    "    # Train Ensemble Model\n",
    "    ensemble_clf.fit(x_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    dump(ensemble_clf, f'GBPUSD_D1_3112_NEWBuy/model_fold_{fold + 1}.joblib')\n",
    "\n",
    "    # Predict on training data\n",
    "    y_train_pred = ensemble_clf.predict(x_train)\n",
    "\n",
    "    # Predict on testing data\n",
    "    y_test_pred = ensemble_clf.predict(x_test)\n",
    "    y_test_prob = ensemble_clf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Apply Custom Threshold on Testing Data\n",
    "    custom_threshold = adjust_threshold(y_test_prob, y_test)\n",
    "    y_test_pred_thresholded = (y_test_prob >= custom_threshold).astype(int)\n",
    "\n",
    "    # Track the best model based on F1 score\n",
    "    if f1_score(y_test, y_test_pred) > best_f1_score:\n",
    "        best_f1_score = f1_score(y_test, y_test_pred)\n",
    "        best_model_fold = fold + 1\n",
    "        best_model = ensemble_clf\n",
    "        best_test_predictions = y_test_pred\n",
    "        best_test_thresholded_predictions = y_test_pred_thresholded\n",
    "        best_test_indices = test_index\n",
    "        best_threshold_value = custom_threshold  # Store the best threshold\n",
    "\n",
    "    # Aggregate metrics across folds if needed\n",
    "    overall_precision_sum_train += precision_score(y_train, y_train_pred)\n",
    "    overall_recall_sum_train += recall_score(y_train, y_train_pred)\n",
    "    overall_f1_sum_train += f1_score(y_train, y_train_pred)\n",
    "\n",
    "    overall_precision_sum_test += precision_score(y_test, y_test_pred)\n",
    "    overall_recall_sum_test += recall_score(y_test, y_test_pred)\n",
    "    overall_f1_sum_test += f1_score(y_test, y_test_pred)\n",
    "\n",
    "    split_num += 1\n",
    "\n",
    "# After cross-validation, calculate average metrics\n",
    "overall_precision_train = overall_precision_sum_train / split_num\n",
    "overall_recall_train = overall_recall_sum_train / split_num\n",
    "overall_f1_train = overall_f1_sum_train / split_num\n",
    "\n",
    "overall_precision_test = overall_precision_sum_test / split_num\n",
    "overall_recall_test = overall_recall_sum_test / split_num\n",
    "overall_f1_test = overall_f1_sum_test / split_num\n",
    "\n",
    "# Save the best model\n",
    "best_model_path = f'GBPUSD_D1_3112_NEWBuy/best_model_fold_{best_model_fold}.joblib'\n",
    "dump(best_model, best_model_path)\n",
    "print(f\"The best model was from fold {best_model_fold} with an F1 score of {best_f1_score:.4f}\")\n",
    "print(f\"Best threshold for this model: {best_threshold_value:.2f}\")\n",
    "print(f\"Model saved to {best_model_path}\")\n",
    "\n",
    "# Save predictions of the best model to a CSV file\n",
    "df_pred = pd.DataFrame(index=best_test_indices)\n",
    "df_pred['prediction'] = best_test_predictions\n",
    "df_pred['prediction_thresholded'] = best_test_thresholded_predictions\n",
    "df_pred['actual'] = y[best_test_indices]\n",
    "df_pred.to_csv('GBPUSD_D1_3112_NEWBuy/best_model_predictions.csv', index=True)\n",
    "\n",
    "print('Overall Training Data Metrics:')\n",
    "print('Precision:', overall_precision_train)\n",
    "print('Recall:', overall_recall_train)\n",
    "print('F1 Score:', overall_f1_train)\n",
    "\n",
    "print('Overall Testing Data Metrics:')\n",
    "print('Precision:', overall_precision_test)\n",
    "print('Recall:', overall_recall_test)\n",
    "print('F1 Score:', overall_f1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b69c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "feature_names = X_df.columns\n",
    "with open('GBPUSD_D1_3112_NEWBuy/feature_names.json', 'w') as f:\n",
    "    json.dump(list(feature_names), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1c6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 1 Training Metrics:\n",
      "Precision: 0.8795, Recall: 0.9818, F1 Score: 0.9278\n",
      "Fold 1 Testing Metrics:\n",
      "Precision: 0.6604, Recall: 0.8061, F1 Score: 0.7260, AUC: 0.7732\n",
      "Custom Threshold: 0.50, BreakEvenRatio: 0.50\n",
      "Fold 2\n",
      "Fold 2 Training Metrics:\n",
      "Precision: 0.8233, Recall: 0.9440, F1 Score: 0.8795\n",
      "Fold 2 Testing Metrics:\n",
      "Precision: 0.6301, Recall: 0.7987, F1 Score: 0.7044, AUC: 0.7861\n",
      "Custom Threshold: 0.50, BreakEvenRatio: 0.50\n",
      "Fold 3\n",
      "Fold 3 Training Metrics:\n",
      "Precision: 0.7831, Recall: 0.9012, F1 Score: 0.8380\n",
      "Fold 3 Testing Metrics:\n",
      "Precision: 0.6190, Recall: 0.7696, F1 Score: 0.6861, AUC: 0.7511\n",
      "Custom Threshold: 0.50, BreakEvenRatio: 0.50\n",
      "Fold 4\n",
      "Fold 4 Training Metrics:\n",
      "Precision: 0.7604, Recall: 0.8807, F1 Score: 0.8161\n",
      "Fold 4 Testing Metrics:\n",
      "Precision: 0.6753, Recall: 0.7305, F1 Score: 0.7018, AUC: 0.7319\n",
      "Custom Threshold: 0.40, BreakEvenRatio: 0.50\n",
      "Fold 5\n",
      "Fold 5 Training Metrics:\n",
      "Precision: 0.7446, Recall: 0.8784, F1 Score: 0.8060\n",
      "Fold 5 Testing Metrics:\n",
      "Precision: 0.5818, Recall: 0.8073, F1 Score: 0.6763, AUC: 0.7616\n",
      "Custom Threshold: 0.50, BreakEvenRatio: 0.50\n",
      "The best model was from fold 1 with an F1 score of 0.7260\n",
      "Model saved to GBPUSD_D1_3112_BuyNEW/best_model_fold_1.joblib\n",
      "Overall Training Data Metrics:\n",
      "Precision: 0.7982, Recall: 0.9172, F1 Score: 0.8535\n",
      "Overall Testing Data Metrics:\n",
      "Precision: 0.6333, Recall: 0.7824, F1 Score: 0.6989\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from joblib import dump\n",
    "\n",
    "# Function to adjust threshold for better prediction\n",
    "def adjust_threshold(probas, target):\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    for threshold in np.arange(0.1, 1.0, 0.1):\n",
    "        preds = (probas >= threshold).astype(int)\n",
    "        f1 = f1_score(target, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold\n",
    "\n",
    "# Setup directories\n",
    "os.makedirs('GBPUSD_D1_3112_Buylatest', exist_ok=True)\n",
    "\n",
    "# TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define models for ensemble with improved hyperparameters\n",
    "clf1 = RandomForestClassifier(\n",
    "    n_estimators=200,  # Increased number of trees\n",
    "    max_depth=25,  # Increased depth of trees\n",
    "    min_samples_split=3,  # Reduced to allow more splits\n",
    "    min_samples_leaf=2,  # Reduced to allow smaller leaves\n",
    "    max_features='sqrt',  # Changed to auto to consider all features\n",
    "    random_state=0,\n",
    "    max_leaf_nodes=20,  # Increased number of leaf nodes\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    ccp_alpha=0.005,  # Reduced complexity parameter\n",
    "    class_weight={0: 10, 1: 20}  # Increased class weight for positive class\n",
    ")\n",
    "\n",
    "clf2 = GradientBoostingClassifier(\n",
    "    n_estimators=200,  # Increased number of boosting stages\n",
    "    learning_rate=0.05,  # Reduced learning rate for finer updates\n",
    "    max_depth=4,  # Increased depth for more complex interactions\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "clf3 = LogisticRegression(\n",
    "    C=0.01,  # Regularization parameter\n",
    "    solver='liblinear'  # Solver for small datasets and binary classification\n",
    ")\n",
    "\n",
    "# Ensemble model - Voting Classifier\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('rf', clf1),\n",
    "    ('gb', clf2),\n",
    "    ('lr', clf3)\n",
    "], voting='soft')  # Use 'soft' voting for better probability estimates\n",
    "\n",
    "# Ensure the time series order is preserved in the split\n",
    "X = X_df.iloc[:, :-1].values\n",
    "y = X_df['b_flag'].values\n",
    "\n",
    "# Initialize variables for aggregating results and tracking the best model\n",
    "overall_precision_sum_train = 0\n",
    "overall_recall_sum_train = 0\n",
    "overall_f1_sum_train = 0\n",
    "overall_precision_sum_test = 0\n",
    "overall_recall_sum_test = 0\n",
    "overall_f1_sum_test = 0\n",
    "best_f1_score = -np.inf\n",
    "best_model_fold = None\n",
    "best_model = None\n",
    "best_test_predictions = None\n",
    "best_test_thresholded_predictions = None\n",
    "best_test_indices = None\n",
    "split_num = 0  # Initialize split_num here\n",
    "\n",
    "# Define BreakEvenRatio\n",
    "break_even_ratio = 0.5  # Example, adjust according to your specific risk/reward ratio\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_df)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Train data\n",
    "    x_train, y_train = X[train_index], y[train_index]\n",
    "    x_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "    # Scale Data\n",
    "    sc_mt = StandardScaler()\n",
    "    x_train = sc_mt.fit_transform(x_train)\n",
    "    x_test = sc_mt.transform(x_test)\n",
    "\n",
    "    # Save scaler\n",
    "    dump(sc_mt, f'GBPUSD_D1_3112_BuyNEW/scaler_fold_{fold + 1}.joblib')\n",
    "\n",
    "    # Train Ensemble Model\n",
    "    ensemble_clf.fit(x_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    dump(ensemble_clf, f'GBPUSD_D1_3112_BuyNEW/model_fold_{fold + 1}.joblib')\n",
    "\n",
    "    # Predict on training data\n",
    "    y_train_pred = ensemble_clf.predict(x_train)\n",
    "    y_train_prob = ensemble_clf.predict_proba(x_train)[:, 1]\n",
    "\n",
    "    # Predict on testing data\n",
    "    y_test_pred = ensemble_clf.predict(x_test)\n",
    "    y_test_prob = ensemble_clf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Apply Custom Threshold on Testing Data\n",
    "    custom_threshold = adjust_threshold(y_test_prob, y_test)\n",
    "    y_test_pred_thresholded = (y_test_prob >= custom_threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics for training and testing\n",
    "    precision_train = precision_score(y_train, y_train_pred)\n",
    "    recall_train = recall_score(y_train, y_train_pred)\n",
    "    f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "    precision_test = precision_score(y_test, y_test_pred)\n",
    "    recall_test = recall_score(y_test, y_test_pred)\n",
    "    f1_test = f1_score(y_test, y_test_pred)\n",
    "    auc_test = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "    # Print metrics for this fold\n",
    "    print(f\"Fold {fold + 1} Training Metrics:\")\n",
    "    print(f\"Precision: {precision_train:.4f}, Recall: {recall_train:.4f}, F1 Score: {f1_train:.4f}\")\n",
    "\n",
    "    print(f\"Fold {fold + 1} Testing Metrics:\")\n",
    "    print(f\"Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1 Score: {f1_test:.4f}, AUC: {auc_test:.4f}\")\n",
    "    print(f\"Custom Threshold: {custom_threshold:.2f}, BreakEvenRatio: {break_even_ratio:.2f}\")\n",
    "\n",
    "    # Track the best model based on F1 score\n",
    "    if f1_score(y_test, y_test_pred) > best_f1_score:\n",
    "        best_f1_score = f1_score(y_test, y_test_pred)\n",
    "        best_model_fold = fold + 1\n",
    "        best_model = ensemble_clf\n",
    "        best_test_predictions = y_test_pred\n",
    "        best_test_thresholded_predictions = y_test_pred_thresholded\n",
    "        best_test_indices = test_index\n",
    "\n",
    "    # Aggregate metrics across folds\n",
    "    overall_precision_sum_train += precision_train\n",
    "    overall_recall_sum_train += recall_train\n",
    "    overall_f1_sum_train += f1_train\n",
    "\n",
    "    overall_precision_sum_test += precision_test\n",
    "    overall_recall_sum_test += recall_test\n",
    "    overall_f1_sum_test += f1_test\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    fold_metrics.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Precision_Train': precision_train,\n",
    "        'Recall_Train': recall_train,\n",
    "        'F1_Train': f1_train,\n",
    "        'Precision_Test': precision_test,\n",
    "        'Recall_Test': recall_test,\n",
    "        'F1_Test': f1_test,\n",
    "        'AUC_Test': auc_test,\n",
    "        'Custom_Threshold': custom_threshold,\n",
    "        'BreakEvenRatio': break_even_ratio\n",
    "    })\n",
    "\n",
    "    split_num += 1\n",
    "\n",
    "# After cross-validation, calculate average metrics\n",
    "overall_precision_train = overall_precision_sum_train / split_num\n",
    "overall_recall_train = overall_recall_sum_train / split_num\n",
    "overall_f1_train = overall_f1_sum_train / split_num\n",
    "\n",
    "overall_precision_test = overall_precision_sum_test / split_num\n",
    "overall_recall_test = overall_recall_sum_test / split_num\n",
    "overall_f1_test = overall_f1_sum_test / split_num\n",
    "\n",
    "# Save the best model\n",
    "best_model_path = f'GBPUSD_D1_3112_BuyNEW/best_model_fold_{best_model_fold}.joblib'\n",
    "dump(best_model, best_model_path)\n",
    "print(f\"The best model was from fold {best_model_fold} with an F1 score of {best_f1_score:.4f}\")\n",
    "print(f\"Model saved to {best_model_path}\")\n",
    "\n",
    "# Save predictions of the best model to a CSV file\n",
    "df_pred = pd.DataFrame(index=best_test_indices)\n",
    "df_pred['prediction'] = best_test_predictions\n",
    "df_pred['prediction_thresholded'] = best_test_thresholded_predictions\n",
    "df_pred['actual'] = y[best_test_indices]\n",
    "df_pred.to_csv('GBPUSD_D1_3112_BuyNEW/best_model_predictions.csv', index=True)\n",
    "\n",
    "print('Overall Training Data Metrics:')\n",
    "print(f'Precision: {overall_precision_train:.4f}, Recall: {overall_recall_train:.4f}, F1 Score: {overall_f1_train:.4f}')\n",
    "\n",
    "print('Overall Testing Data Metrics:')\n",
    "print(f'Precision: {overall_precision_test:.4f}, Recall: {overall_recall_test:.4f}, F1 Score: {overall_f1_test:.4f}')\n",
    "\n",
    "# Convert fold metrics to a DataFrame and save to a CSV file\n",
    "df_fold_metrics = pd.DataFrame(fold_metrics)\n",
    "df_fold_metrics.to_csv('GBPUSD_D1_3112_BuyNEW/fold_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef2edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
