{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbb1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MetaTrader5 as mt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import talib\n",
    "from talipp.indicators import EMA, SMA, Stoch, DPO\n",
    "from joblib import dump\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, confusion_matrix, classification_report\n",
    "from own_functions import *\n",
    "import os\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, make_forecasting_frame\n",
    "from tsfresh.utilities.dataframe_functions import impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1149e9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.initialize()\n",
    "login = 51708234\n",
    "password =\"4bM&wuVJcBTnjV\"\n",
    "server = \"ICMarketsEU-Demo\"\n",
    "mt.login(login,password,server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cca420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" csv_file_path = 'ICMT5EURUSD2010_3112_D1.csv'  # Specify your desired path\\ndf.to_csv(csv_file_path, index=False)\\ndf \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = \"USDCAD\"\n",
    "timeframe = mt.TIMEFRAME_D1\n",
    "ohlc_data = pd.DataFrame(mt.copy_rates_range(symbol, timeframe, datetime(2010, 1, 1), datetime(2023, 12, 31)))\n",
    "ohlc_data['time'] = pd.to_datetime(ohlc_data['time'], unit='s')\n",
    "df = ohlc_data[['time', 'open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "# Indicators\n",
    "df['WILLR_15'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=15)\n",
    "df['WILLR_23'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=23)\n",
    "df['WILLR_42'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=42)\n",
    "df['WILLR_145'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=145)\n",
    "\n",
    "# Buy & Sell Flags\n",
    "df['b_flag'] = 0\n",
    "df['s_flag'] = 0\n",
    "\n",
    "# Dropping NaN values and resetting index\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\"\"\" csv_file_path = 'ICMT5EURUSD2010_3112_D1.csv'  # Specify your desired path\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "df \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1ee607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Candle: 0.008802622527944972\n"
     ]
    }
   ],
   "source": [
    "StopLoss = 1\n",
    "TakeProfit = 2\n",
    "BreakEvenRatio=StopLoss/(StopLoss+TakeProfit)\n",
    "label_data(df,[StopLoss],[TakeProfit],80,symbol,False)\n",
    "#print('BreatEvenRatio:', BreakEvenRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36f2e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 1s:\n",
      "b_flag: 1226\n",
      "s_flag: 1000\n",
      "\n",
      "Counts in segments of 100% data:\n",
      "b_flag: 1226\n",
      "s_flag: 1000\n",
      "\n",
      "Counts in intervals of 10%:\n",
      "0% - 10%: b_flag=105, s_flag=109\n",
      "10% - 20%: b_flag=100, s_flag=127\n",
      "20% - 30%: b_flag=155, s_flag=97\n",
      "30% - 40%: b_flag=161, s_flag=63\n",
      "40% - 50%: b_flag=107, s_flag=111\n",
      "50% - 60%: b_flag=114, s_flag=115\n",
      "60% - 70%: b_flag=117, s_flag=93\n",
      "70% - 80%: b_flag=102, s_flag=139\n",
      "80% - 90%: b_flag=163, s_flag=78\n",
      "90% - 100%: b_flag=102, s_flag=68\n",
      "100% - 110%: b_flag=0, s_flag=0\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of 1s in b_flag and s_flag columns\n",
    "total_b_flags = df['b_flag'].sum()\n",
    "total_s_flags = df['s_flag'].sum()\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate counts in segments of complete 100% data\n",
    "count_100_b_flags = total_b_flags\n",
    "count_100_s_flags = total_s_flags\n",
    "\n",
    "# Calculate counts in intervals of 10%\n",
    "interval_counts = []\n",
    "for i in range(0, 101, 10):\n",
    "    start_idx = int(i / 100 * total_rows)\n",
    "    end_idx = int((i + 10) / 100 * total_rows)\n",
    "    \n",
    "    interval_b_flags = df['b_flag'].iloc[start_idx:end_idx].sum()\n",
    "    interval_s_flags = df['s_flag'].iloc[start_idx:end_idx].sum()\n",
    "    \n",
    "    interval_counts.append((f'{i}% - {i+10}%', interval_b_flags, interval_s_flags))\n",
    "\n",
    "# Print results\n",
    "print(\"Total number of 1s:\")\n",
    "print(f\"b_flag: {total_b_flags}\")\n",
    "print(f\"s_flag: {total_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in segments of 100% data:\")\n",
    "print(f\"b_flag: {count_100_b_flags}\")\n",
    "print(f\"s_flag: {count_100_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in intervals of 10%:\")\n",
    "for interval, count_b, count_s in interval_counts:\n",
    "    print(f\"{interval}: b_flag={count_b}, s_flag={count_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af671f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 30/30 [00:06<00:00,  4.88it/s]\n",
      "Feature Extraction: 100%|██████████| 30/30 [01:25<00:00,  2.84s/it]\n",
      "Rolling: 100%|██████████| 30/30 [00:06<00:00,  4.36it/s]\n",
      "Feature Extraction: 100%|██████████| 30/30 [01:23<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WILLR_42__mean_second_derivative_central', 'WILLR_15__mean_second_derivative_central', 'WILLR_15', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_7', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_7', 'WILLR_23', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_10', 'WILLR_42__fft_coefficient__attr_\"real\"__coeff_10', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_6', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_8', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_6', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_9', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_8', 'WILLR_42', 'WILLR_15__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"max\"', 'WILLR_42__fft_coefficient__attr_\"real\"__coeff_9', 'WILLR_42__agg_linear_trend__attr_\"rvalue\"__chunk_len_10__f_agg_\"mean\"', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_7', 'WILLR_15__agg_linear_trend__attr_\"stderr\"__chunk_len_5__f_agg_\"max\"', 'WILLR_15__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"mean\"', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_5', 'WILLR_15__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"max\"', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_6', 'WILLR_42__agg_linear_trend__attr_\"rvalue\"__chunk_len_10__f_agg_\"min\"', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_8', 'WILLR_15__agg_linear_trend__attr_\"rvalue\"__chunk_len_10__f_agg_\"mean\"', 'WILLR_42__fft_coefficient__attr_\"real\"__coeff_8', 'WILLR_42__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"mean\"', 'WILLR_15__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"min\"', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_5', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.0', 'WILLR_15__mean_change', 'WILLR_42__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"min\"', 'WILLR_42__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"max\"', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.2__ql_0.0', 'WILLR_42__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"min\"', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_8', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_7', 'WILLR_42__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"max\"', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_4', 'WILLR_15__symmetry_looking__r_0.05', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_6', 'WILLR_42__agg_linear_trend__attr_\"stderr\"__chunk_len_5__f_agg_\"max\"', 'WILLR_42__mean_change', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_1.0__ql_0.0', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_5', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_5', 'WILLR_15__fft_coefficient__attr_\"angle\"__coeff_8', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_9', 'WILLR_15__agg_linear_trend__attr_\"rvalue\"__chunk_len_10__f_agg_\"min\"', 'WILLR_42__agg_linear_trend__attr_\"rvalue\"__chunk_len_5__f_agg_\"mean\"', 'WILLR_15__large_standard_deviation__r_0.30000000000000004', 'WILLR_15__approximate_entropy__m_2__r_0.3', 'WILLR_42__partial_autocorrelation__lag_2', 'WILLR_42__large_standard_deviation__r_0.30000000000000004', 'WILLR_15__autocorrelation__lag_2', 'WILLR_42__fft_coefficient__attr_\"imag\"__coeff_4', 'WILLR_42__fft_coefficient__attr_\"real\"__coeff_3', 'WILLR_42__agg_linear_trend__attr_\"rvalue\"__chunk_len_5__f_agg_\"min\"', 'WILLR_15__partial_autocorrelation__lag_2', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_3', 'WILLR_15__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.4', 'WILLR_15__fft_coefficient__attr_\"imag\"__coeff_9', 'WILLR_15__agg_linear_trend__attr_\"stderr\"__chunk_len_10__f_agg_\"min\"', 'WILLR_15__has_duplicate', 'WILLR_15__percentage_of_reoccurring_datapoints_to_all_datapoints', 'WILLR_15__percentage_of_reoccurring_values_to_all_values', 'WILLR_15__ratio_value_number_to_time_series_length', 'WILLR_15__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"max\"', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.4__ql_0.0', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.2__ql_0.0', 'WILLR_15__change_quantiles__f_agg_\"var\"__isabs_False__qh_1.0__ql_0.8', 'WILLR_42__fft_coefficient__attr_\"angle\"__coeff_4', 'WILLR_15__autocorrelation__lag_4', 'WILLR_15__sum_of_reoccurring_values', 'WILLR_15__sum_of_reoccurring_data_points', 'WILLR_15__symmetry_looking__r_0.1', 'WILLR_15__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', 'WILLR_15__fft_coefficient__attr_\"real\"__coeff_2', 'WILLR_15__permutation_entropy__dimension_3__tau_1', 'WILLR_15__agg_linear_trend__attr_\"rvalue\"__chunk_len_5__f_agg_\"mean\"', 'WILLR_15__lempel_ziv_complexity__bins_10', 'WILLR_15__linear_trend__attr_\"stderr\"', 'WILLR_42__kurtosis', 'WILLR_42__permutation_entropy__dimension_3__tau_1', 'WILLR_15__linear_trend__attr_\"pvalue\"', 'WILLR_15__longest_strike_below_mean', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.0', 'WILLR_42__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.8__ql_0.0', 'WILLR_15__energy_ratio_by_chunks__num_segments_10__segment_focus_8', 'WILLR_42__index_mass_quantile__q_0.9', 'WILLR_15__autocorrelation__lag_3', 'open', 'WILLR_42__agg_linear_trend__attr_\"slope\"__chunk_len_5__f_agg_\"mean\"', 'WILLR_15__change_quantiles__f_agg_\"var\"__isabs_True__qh_1.0__ql_0.8', 's_flag']\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "df.drop(columns=['b_flag'], inplace=True)\n",
    "\n",
    "selected_signal_1 = 'WILLR_15'\n",
    "df_melted_1 = df[['time', selected_signal_1]].copy()\n",
    "df_melted_1[\"Symbols\"] = symbol\n",
    "\n",
    "df_rolled_1 = roll_time_series(df_melted_1, column_id=\"Symbols\", column_sort=\"time\",\n",
    "                               max_timeshift=20, min_timeshift=5)\n",
    "\n",
    "X1 = extract_features(df_rolled_1.drop(\"Symbols\", axis=1), \n",
    "                      column_id=\"id\", column_sort=\"time\", column_value=selected_signal_1, \n",
    "                      impute_function=impute, show_warnings=False)\n",
    "\n",
    "X1 = X1.set_index(X1.index.map(lambda x: x[1]), drop=True)\n",
    "X1.index.name = \"time\"\n",
    "X1 = X1.dropna()\n",
    "\n",
    "selected_signal_2 = 'WILLR_42'\n",
    "df_melted_2 = df[['time', selected_signal_2]].copy()\n",
    "df_melted_2[\"Symbols\"] = symbol\n",
    "\n",
    "df_rolled_2 = roll_time_series(df_melted_2, column_id=\"Symbols\", column_sort=\"time\",\n",
    "                               max_timeshift=20, min_timeshift=5)\n",
    "\n",
    "X2 = extract_features(df_rolled_2.drop(\"Symbols\", axis=1), \n",
    "                      column_id=\"id\", column_sort=\"time\", column_value=selected_signal_2, \n",
    "                      impute_function=impute, show_warnings=False)\n",
    "\n",
    "X2 = X2.set_index(X2.index.map(lambda x: x[1]), drop=True)\n",
    "X2.index.name = \"time\"\n",
    "X2 = X2.dropna()\n",
    "\n",
    "X = pd.concat([X1, X2], axis=1, join='inner')\n",
    "X = X.dropna()\n",
    "\n",
    "# Align indices\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.set_index('time')\n",
    "df = df[df.index.isin(X.index)]\n",
    "\n",
    "X = pd.concat([X, df], axis=1, join='inner')\n",
    "\n",
    "# Ensure b_flag is at the end after feature selection\n",
    "X_df = select_features(X, X['s_flag'])\n",
    "X_df = X_df[[col for col in X_df if col != 's_flag'] + ['s_flag']]\n",
    "\n",
    "# Get the list of selected feature names\n",
    "selected_feature_names_X = list(X_df.columns)\n",
    "\n",
    "# Combine lists if you need a single list for all selected features\n",
    "\n",
    "print(selected_feature_names_X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef883039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[432 120]\n",
      " [ 74  71]]\n",
      "WIN/LOSS-Diff: 3.84 %\n",
      "sum_fp: 120\n",
      "sum_tp: 71\n",
      "precision: 0.3717277486910995\n",
      "Ratio total: 37.17\n",
      "BreakEvenRatio: 0.33\n",
      "____________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sum_fp = 0\n",
    "sum_tp = 0\n",
    "\n",
    "split = int(0.80 * len(X_df))\n",
    "\n",
    "train_data, test_data = X_df.iloc[:split], X_df.iloc[split:]\n",
    "\n",
    "# Train data\n",
    "x_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data['s_flag'].values\n",
    "# Test data\n",
    "x_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data['s_flag'].values\n",
    "\n",
    "# Scale Data\n",
    "sc_mt = StandardScaler()\n",
    "x_train = sc_mt.fit_transform(x_train)\n",
    "x_test = sc_mt.transform(x_test)\n",
    "\n",
    "os.makedirs('Dump_GBPUSD_D1_3112_Sell', exist_ok=True)\n",
    "dump(sc_mt, 'Dump_GBPUSD_D1_3112_Sell/scaler.joblib')\n",
    "\n",
    "# Hyperparameter\n",
    "n_estimators = 180\n",
    "class_weight = {0: 6, 1: 1}\n",
    "max_features = 'sqrt'\n",
    "random_state = 1\n",
    "\n",
    "# Initialise RandomForestClassifier\n",
    "rf_classifier_mt = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    class_weight=class_weight,\n",
    "    max_features=max_features,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "rf_classifier_mt.fit(x_train, y_train)\n",
    "\n",
    "dump(rf_classifier_mt, 'Dump_GBPUSD_D1_3112_Sell/model.joblib')\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_classifier_mt.predict(x_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "false_positives = cm[0][1]\n",
    "true_positives = cm[1][1]\n",
    "\n",
    "sum_fp += false_positives\n",
    "sum_tp += true_positives\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print('WIN/LOSS-Diff:', round(100 * (precision - BreakEvenRatio), 2), '%')\n",
    "print('sum_fp:', sum_fp)\n",
    "print('sum_tp:', sum_tp)\n",
    "print('precision:', precision)\n",
    "print('Ratio total:', round(100 * (sum_tp / (sum_fp + sum_tp)), 2))\n",
    "print('BreakEvenRatio:', round(BreakEvenRatio, 2))\n",
    "print('____________________________________________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f65221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(index=test_data.index)\n",
    "df_pred['prediction'] = y_pred\n",
    "df_pred.to_csv('predictGBPUSD_D1_3112_Sell.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bb1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"GBPUSD\"\n",
    "timeframe = mt.TIMEFRAME_D1\n",
    "ohlc_data = pd.DataFrame(mt.copy_rates_range(symbol, timeframe, datetime(2010, 1, 1), datetime(2023, 12, 31)))\n",
    "ohlc_data['time'] = pd.to_datetime(ohlc_data['time'], unit='s')\n",
    "df = ohlc_data[['time', 'open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "# Indicators\n",
    "df['WILLR_15'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=15)\n",
    "df['WILLR_23'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=23)\n",
    "df['WILLR_42'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=42)\n",
    "df['WILLR_145'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=145)\n",
    "\n",
    "# Buy & Sell Flags\n",
    "df['b_flag'] = 0\n",
    "df['s_flag'] = 0\n",
    "\n",
    "# Dropping NaN values and resetting index\n",
    "df = df.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246cbf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import os\\nfrom joblib import dump\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import confusion_matrix, precision_score, make_scorer\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\\nfrom imblearn.over_sampling import SMOTE\\n\\n# Assuming X_df is your dataframe\\n\\n# Set test size percentage (e.g., 20% of the data)\\ntest_size_percentage = 0.20\\nsplit = int((1 - test_size_percentage) * len(X_df))\\n\\ntrain_data, test_data = X_df.iloc[:split], X_df.iloc[split:]\\n\\n# Train data\\nx_train = train_data.iloc[:, :-1].values\\ny_train = train_data[\\'s_flag\\'].values\\n# Test data\\nx_test = test_data.iloc[:, :-1].values\\ny_test = test_data[\\'s_flag\\'].values\\n\\n# Scale Data\\nsc_mt = StandardScaler()\\nx_train = sc_mt.fit_transform(x_train)\\nx_test = sc_mt.transform(x_test)\\n\\nos.makedirs(\\'Dump_GBPUSD_D1_3112_Sell\\', exist_ok=True)\\ndump(sc_mt, \\'Dump_GBPUSD_D1_3112_Sell/scaler.joblib\\')\\n\\n# Resampling using SMOTE\\nsmote = SMOTE(random_state=8)\\nx_train_res, y_train_res = smote.fit_resample(x_train, y_train)\\n\\n# Hyperparameter tuning using TimeSeriesSplit\\nparam_grid = {\\n    \\'n_estimators\\': [100, 200, 300,400,500,600,700,800,900],\\n    \\'class_weight\\': [{0: 5, 1: 10}, {0: 3, 1: 7}],\\n    \\'max_features\\': [\\'auto\\', \\'log2\\'],\\n    \\'random_state\\': [0,8,10,42,60,80,100,150]\\n}\\n\\n# Custom scoring function to prioritize true positives and limit false positives\\ndef custom_scorer(y_true, y_pred):\\n    cm = confusion_matrix(y_true, y_pred)\\n    false_positives = cm[0][1]\\n    true_positives = cm[1][1]\\n    if true_positives >= 50 and false_positives < 15:\\n        return 1  # High score for models meeting the criteria\\n    else:\\n        return 0  # Low score for models not meeting the criteria\\n\\nscorer = make_scorer(custom_scorer, greater_is_better=True)\\n\\n# Use TimeSeriesSplit for cross-validation\\ntscv = TimeSeriesSplit(n_splits=5)\\n\\nrf_classifier_mt = RandomForestClassifier()\\n\\ngrid_search = GridSearchCV(estimator=rf_classifier_mt, param_grid=param_grid, scoring=scorer, cv=tscv, verbose=1, n_jobs=-1)\\ngrid_search.fit(x_train_res, y_train_res)\\n\\nbest_model = grid_search.best_estimator_\\n\\n# Print the best model parameters\\nprint(\"Best model parameters:\")\\nprint(grid_search.best_params_)\\n\\ndump(best_model, \\'Dump_GBPUSD_D1_3112_Sell/model.joblib\\')\\n\\n# Predict\\ny_pred = best_model.predict(x_test)\\n\\nprint(\"Confusion Matrix:\")\\ncm = confusion_matrix(y_test, y_pred)\\nprint(cm)\\n\\nfalse_positives = cm[0][1]\\ntrue_positives = cm[1][1]\\n\\nsum_fp = false_positives\\nsum_tp = true_positives\\n\\nprecision = precision_score(y_test, y_pred)\\n\\nprint(\\'WIN/LOSS-Diff:\\', round(100 * (precision - BreakEvenRatio), 2), \\'%\\')\\nprint(\\'sum_fp:\\', sum_fp)\\nprint(\\'sum_tp:\\', sum_tp)\\nprint(\\'precision:\\', precision)\\nprint(\\'Ratio total:\\', round(100 * (sum_tp / (sum_fp + sum_tp)), 2))\\nprint(\\'BreakEvenRatio:\\', round(BreakEvenRatio, 2))\\nprint(\\'____________________________________________________________________________________________________________________________\\')\\n '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import os\n",
    "from joblib import dump\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming X_df is your dataframe\n",
    "\n",
    "# Set test size percentage (e.g., 20% of the data)\n",
    "test_size_percentage = 0.20\n",
    "split = int((1 - test_size_percentage) * len(X_df))\n",
    "\n",
    "train_data, test_data = X_df.iloc[:split], X_df.iloc[split:]\n",
    "\n",
    "# Train data\n",
    "x_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data['s_flag'].values\n",
    "# Test data\n",
    "x_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data['s_flag'].values\n",
    "\n",
    "# Scale Data\n",
    "sc_mt = StandardScaler()\n",
    "x_train = sc_mt.fit_transform(x_train)\n",
    "x_test = sc_mt.transform(x_test)\n",
    "\n",
    "os.makedirs('Dump_GBPUSD_D1_3112_Sell', exist_ok=True)\n",
    "dump(sc_mt, 'Dump_GBPUSD_D1_3112_Sell/scaler.joblib')\n",
    "\n",
    "# Resampling using SMOTE\n",
    "smote = SMOTE(random_state=8)\n",
    "x_train_res, y_train_res = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# Hyperparameter tuning using TimeSeriesSplit\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300,400,500,600,700,800,900],\n",
    "    'class_weight': [{0: 5, 1: 10}, {0: 3, 1: 7}],\n",
    "    'max_features': ['auto', 'log2'],\n",
    "    'random_state': [0,8,10,42,60,80,100,150]\n",
    "}\n",
    "\n",
    "# Custom scoring function to prioritize true positives and limit false positives\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    false_positives = cm[0][1]\n",
    "    true_positives = cm[1][1]\n",
    "    if true_positives >= 50 and false_positives < 15:\n",
    "        return 1  # High score for models meeting the criteria\n",
    "    else:\n",
    "        return 0  # Low score for models not meeting the criteria\n",
    "\n",
    "scorer = make_scorer(custom_scorer, greater_is_better=True)\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rf_classifier_mt = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_classifier_mt, param_grid=param_grid, scoring=scorer, cv=tscv, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(x_train_res, y_train_res)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best model parameters\n",
    "print(\"Best model parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "dump(best_model, 'Dump_GBPUSD_D1_3112_Sell/model.joblib')\n",
    "\n",
    "# Predict\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "false_positives = cm[0][1]\n",
    "true_positives = cm[1][1]\n",
    "\n",
    "sum_fp = false_positives\n",
    "sum_tp = true_positives\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print('WIN/LOSS-Diff:', round(100 * (precision - BreakEvenRatio), 2), '%')\n",
    "print('sum_fp:', sum_fp)\n",
    "print('sum_tp:', sum_tp)\n",
    "print('precision:', precision)\n",
    "print('Ratio total:', round(100 * (sum_tp / (sum_fp + sum_tp)), 2))\n",
    "print('BreakEvenRatio:', round(BreakEvenRatio, 2))\n",
    "print('____________________________________________________________________________________________________________________________')\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
