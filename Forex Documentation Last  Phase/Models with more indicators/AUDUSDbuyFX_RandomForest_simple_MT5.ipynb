{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b814e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Candle: 0.008151498996847221\n"
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "from talipp.indicators import EMA, SMA, Stoch, DPO\n",
    "from joblib import dump\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, confusion_matrix, classification_report, recall_score, accuracy_score, f1_score, roc_auc_score\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from own_functions import label_data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hardcoded credentials - as requested\n",
    "login = 51708234\n",
    "password = \"4bM&wuVJcBTnjV\"\n",
    "server = \"ICMarketsEU-Demo\"\n",
    "\n",
    "mt.initialize()\n",
    "mt.login(login, password, server)\n",
    "\n",
    "symbol = \"AUDUSD\"\n",
    "timeframe = mt.TIMEFRAME_D1\n",
    "start_date = datetime(2010, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "StopLoss = 1\n",
    "TakeProfit = 1\n",
    "BreakEvenRatio = StopLoss / (StopLoss + TakeProfit)\n",
    "\n",
    "def add_rolling_features(df, window):\n",
    "    df['rolling_mean_open'] = df['open'].rolling(window=window).mean()\n",
    "    df['rolling_std_open'] = df['open'].rolling(window=window).std()\n",
    "    df['rolling_mean_close'] = df['close'].rolling(window=window).mean()\n",
    "    df['rolling_std_close'] = df['close'].rolling(window=window).std()\n",
    "    df['rolling_mean_high'] = df['high'].rolling(window=window).mean()\n",
    "    df['rolling_std_high'] = df['high'].rolling(window=window).std()\n",
    "    df['rolling_mean_low'] = df['low'].rolling(window=window).mean()\n",
    "    df['rolling_std_low'] = df['low'].rolling(window=window).std()\n",
    "    return df\n",
    "\n",
    "def add_lag_features(df, lags):\n",
    "    for lag in lags:\n",
    "        df[f'open_lag_{lag}'] = df['open'].shift(lag)\n",
    "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "        df[f'high_lag_{lag}'] = df['high'].shift(lag)\n",
    "        df[f'low_lag_{lag}'] = df['low'].shift(lag)\n",
    "    return df\n",
    "\n",
    "def extract_rolling_features(df, signal, symbol, max_shift=20, min_shift=5):\n",
    "    df_melted = df[['time', signal]].copy()\n",
    "    df_melted[\"Symbols\"] = symbol\n",
    "    df_rolled = roll_time_series(df_melted, column_id=\"Symbols\", column_sort=\"time\",\n",
    "                                 max_timeshift=max_shift, min_timeshift=min_shift)\n",
    "    X = extract_features(df_rolled.drop(\"Symbols\", axis=1),\n",
    "                         column_id=\"id\", column_sort=\"time\", column_value=signal,\n",
    "                         impute_function=impute, show_warnings=False)\n",
    "    X = X.set_index(X.index.map(lambda x: x[1]), drop=True)\n",
    "    X.index.name = \"time\"\n",
    "    return X.dropna()\n",
    "\n",
    "# Fetch historical data\n",
    "ohlc_data = pd.DataFrame(mt.copy_rates_range(symbol, timeframe, start_date, end_date))\n",
    "ohlc_data['time'] = pd.to_datetime(ohlc_data['time'], unit='s')\n",
    "df = ohlc_data[['time', 'open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "df['EMA_9'] = talib.EMA(df['close'], timeperiod=9)\n",
    "df['EMA_21'] = talib.EMA(df['close'], timeperiod=21)\n",
    "df['EMA_50'] = talib.EMA(df['close'], timeperiod=50)\n",
    "\n",
    "df['RSI_9'] = talib.RSI(df['close'], timeperiod=9)\n",
    "df['RSI_14'] = talib.RSI(df['close'], timeperiod=14)\n",
    "df['RSI_21'] = talib.RSI(df['close'], timeperiod=21)\n",
    "\n",
    "df['WILLR_15'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=15)\n",
    "df['WILLR_23'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=23)\n",
    "df['WILLR_42'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=42)\n",
    "df['WILLR_145'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=145)\n",
    "\n",
    "df['SAR'] = talib.SAR(df['high'], df['low'], acceleration=0.02, maximum=0.2)\n",
    "\n",
    "df['BB_upper'], df['BB_middle'], df['BB_lower'] = talib.BBANDS(df['close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "df['BB_width'] = df['BB_upper'] - df['BB_lower']\n",
    "\n",
    "df['MACD'], df['MACD_signal'], df['MACD_hist'] = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "df['CCI_14'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "\n",
    "df = add_rolling_features(df, window=5)\n",
    "df = add_lag_features(df, lags=[1, 2, 3, 4, 5])\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df['b_flag'] = 0\n",
    "df['s_flag'] = 0\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "#csv_file_path = 'EURUSD_D1_2010to101124.csv'  # Specify your desired path\n",
    "#df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "label_data(df, [StopLoss], [TakeProfit], 80, symbol, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "324e96de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 1s:\n",
      "b_flag: 1713\n",
      "s_flag: 1671\n",
      "\n",
      "Counts in segments of 100% data:\n",
      "b_flag: 1713\n",
      "s_flag: 1671\n",
      "\n",
      "Counts in intervals of 10%:\n",
      "0% - 10%: b_flag=177, s_flag=167\n",
      "10% - 20%: b_flag=170, s_flag=176\n",
      "20% - 30%: b_flag=177, s_flag=169\n",
      "30% - 40%: b_flag=161, s_flag=184\n",
      "40% - 50%: b_flag=182, s_flag=163\n",
      "50% - 60%: b_flag=184, s_flag=165\n",
      "60% - 70%: b_flag=172, s_flag=177\n",
      "70% - 80%: b_flag=206, s_flag=139\n",
      "80% - 90%: b_flag=151, s_flag=198\n",
      "90% - 100%: b_flag=133, s_flag=133\n",
      "100% - 110%: b_flag=0, s_flag=0\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of 1s in b_flag and s_flag columns\n",
    "total_b_flags = df['b_flag'].sum()\n",
    "total_s_flags = df['s_flag'].sum()\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate counts in segments of complete 100% data\n",
    "count_100_b_flags = total_b_flags\n",
    "count_100_s_flags = total_s_flags\n",
    "\n",
    "# Calculate counts in intervals of 10%\n",
    "interval_counts = []\n",
    "for i in range(0, 101, 10):\n",
    "    start_idx = int(i / 100 * total_rows)\n",
    "    end_idx = int((i + 10) / 100 * total_rows)\n",
    "    \n",
    "    interval_b_flags = df['b_flag'].iloc[start_idx:end_idx].sum()\n",
    "    interval_s_flags = df['s_flag'].iloc[start_idx:end_idx].sum()\n",
    "    \n",
    "    interval_counts.append((f'{i}% - {i+10}%', interval_b_flags, interval_s_flags))\n",
    "\n",
    "# Print results\n",
    "print(\"Total number of 1s:\")\n",
    "print(f\"b_flag: {total_b_flags}\")\n",
    "print(f\"s_flag: {total_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in segments of 100% data:\")\n",
    "print(f\"b_flag: {count_100_b_flags}\")\n",
    "print(f\"s_flag: {count_100_s_flags}\")\n",
    "\n",
    "print(\"\\nCounts in intervals of 10%:\")\n",
    "for interval, count_b, count_s in interval_counts:\n",
    "    print(f\"{interval}: b_flag={count_b}, s_flag={count_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92bdd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 10.35it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:36<00:00,  1.10it/s]\n",
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 11.08it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:38<00:00,  1.03it/s]\n",
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 11.26it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:40<00:00,  1.01s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 10.75it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:41<00:00,  1.04s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 10.22it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:43<00:00,  1.09s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 10.26it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:43<00:00,  1.09s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 10.40it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:44<00:00,  1.11s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.71it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:44<00:00,  1.10s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:03<00:00, 10.15it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:43<00:00,  1.09s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.23it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:43<00:00,  1.10s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.88it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:44<00:00,  1.11s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.91it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:43<00:00,  1.09s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.50it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:44<00:00,  1.12s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.76it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:45<00:00,  1.14s/it]\n",
      "Rolling: 100%|██████████| 40/40 [00:04<00:00,  9.91it/s]\n",
      "Feature Extraction: 100%|██████████| 40/40 [00:44<00:00,  1.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' original_index = X_df.index\\nX_df = X_df.shift(periods=1, axis=0)\\nX_df.index = original_index\\nX_df = X_df.dropna() '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['s_flag'], inplace=True)\n",
    "\n",
    "X1 = extract_rolling_features(df, 'WILLR_15', symbol)\n",
    "X2 = extract_rolling_features(df, 'WILLR_42', symbol)\n",
    "X3 = extract_rolling_features(df, 'RSI_14', symbol)\n",
    "X4 = extract_rolling_features(df, 'MACD_hist', symbol)\n",
    "X5 = extract_rolling_features(df, 'EMA_9', symbol)\n",
    "X6 = extract_rolling_features(df, 'EMA_21', symbol)\n",
    "X7 = extract_rolling_features(df, 'EMA_50', symbol)\n",
    "X9 = extract_rolling_features(df, 'RSI_9', symbol)\n",
    "X10 = extract_rolling_features(df, 'RSI_21', symbol)\n",
    "X11 = extract_rolling_features(df, 'WILLR_23', symbol)\n",
    "X12 = extract_rolling_features(df, 'WILLR_145', symbol)\n",
    "X13 = extract_rolling_features(df, 'SAR', symbol)\n",
    "X14 = extract_rolling_features(df, 'BB_width', symbol)\n",
    "X15 = extract_rolling_features(df, 'MACD_signal', symbol)\n",
    "X16 = extract_rolling_features(df, 'CCI_14', symbol)\n",
    "\n",
    "# Combine all extracted features\n",
    "X = pd.concat([X1, X2, X3, X4, X5, X6, X7, X9, X10, X11, X12, X13, X14, X15, X16], axis=1, join='inner').dropna()\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.set_index('time')\n",
    "X = X[X.index.isin(df.index)]\n",
    "X = pd.concat([df,X], axis=1, join='inner')\n",
    "\n",
    "features = X.drop(columns=['b_flag'])\n",
    "target = X['b_flag']\n",
    "\n",
    "# Shift features by 1 period to use t-1 features for t target\n",
    "features_shifted = features.shift(periods=1, axis=0)\n",
    "\n",
    "# Align the shifted features with the target index\n",
    "aligned_features = features_shifted.loc[target.index]\n",
    "\n",
    "# Combine the shifted features with the target\n",
    "X_df = pd.concat([aligned_features, target], axis=1)\n",
    "\n",
    "# Drop any rows with NaN values resulting from the shift\n",
    "X_df = X_df.dropna()\n",
    "\n",
    "X_df = select_features(X, X['b_flag'])\n",
    "X_df = X_df[[col for col in X_df if col != 'b_flag'] + ['b_flag']]\n",
    "\n",
    "correlation_matrix = X_df.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "high_correlation_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "X_df = X_df.drop(columns=high_correlation_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de92c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before PCA: 288\n",
      "Number of features after PCA: 15\n",
      "Confusion Matrix:\n",
      "[[341  73]\n",
      " [184  99]]\n",
      "Accuracy: 0.6313\n",
      "Precision: 0.5756\n",
      "Recall: 0.3498\n",
      "F1 Score: 0.4352\n",
      "ROC-AUC: 0.6591\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.82      0.73       414\n",
      "           1       0.58      0.35      0.44       283\n",
      "\n",
      "    accuracy                           0.63       697\n",
      "   macro avg       0.61      0.59      0.58       697\n",
      "weighted avg       0.62      0.63      0.61       697\n",
      "\n",
      "WIN/LOSS-Diff: 7.56 %\n",
      "False Positives: 73\n",
      "True Positives: 99\n",
      "Ratio total: 57.56\n",
      "BreakEvenRatio: 0.5\n",
      "____________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "split = int(0.80 * len(X_df))\n",
    "train_data, test_data = X_df.iloc[:split], X_df.iloc[split:]\n",
    "\n",
    "x_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data['b_flag'].values\n",
    "x_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data['b_flag'].values\n",
    "\n",
    "sc_mt = StandardScaler()\n",
    "x_train = sc_mt.fit_transform(x_train)\n",
    "x_test = sc_mt.transform(x_test)\n",
    "\n",
    "print(\"Number of features before PCA:\", x_train.shape[1])\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "\n",
    "pca = PCA(n_components=15, svd_solver='randomized', random_state=0)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "print(\"Number of features after PCA:\", x_train.shape[1])\n",
    "\n",
    "n_estimators = 150\n",
    "class_weight = {0: 1, 1: 5}\n",
    "max_features = 'sqrt'\n",
    "random_state = 0\n",
    "\n",
    "rf_classifier_mt = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    class_weight=class_weight,\n",
    "    max_features=max_features,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "rf_classifier_mt.fit(x_train, y_train)\n",
    "y_pred_proba = rf_classifier_mt.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Add probability threshold for predicting class 1\n",
    "threshold = 0.6  # Adjust as needed to increase precision\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "false_positives = conf_matrix[0][1]\n",
    "true_positives = conf_matrix[1][1]\n",
    "\n",
    "precision = precision_score(y_test, y_pred) if (false_positives+true_positives) > 0 else 0\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print('Accuracy:', round(accuracy, 4))\n",
    "print('Precision:', round(precision, 4))\n",
    "print('Recall:', round(recall, 4))\n",
    "print('F1 Score:', round(f1, 4))\n",
    "print('ROC-AUC:', round(roc_auc, 4))\n",
    "print('Classification Report:\\n', classification_rep)\n",
    "print('WIN/LOSS-Diff:', round(100 * (precision - BreakEvenRatio), 2), '%')\n",
    "print('False Positives:', false_positives)\n",
    "print('True Positives:', true_positives)\n",
    "if (false_positives + true_positives) > 0:\n",
    "    print('Ratio total:', round(100 * (true_positives / (false_positives + true_positives)), 2))\n",
    "print('BreakEvenRatio:', round(BreakEvenRatio, 2))\n",
    "print('____________________________________________________________________________________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36887e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('AUDUSD_D1_2023buy', exist_ok=True)\n",
    "# Save the scaler\n",
    "dump(sc_mt, 'AUDUSD_D1_2023buy/scaler.joblib')\n",
    "dump(rf_classifier_mt, 'AUDUSD_D1_2023buy/model.joblib')\n",
    "dump(pca,'AUDUSD_D1_2023buy/pca.joblib')\n",
    "\n",
    "import json\n",
    "feature_names = [col for col in X_df.columns if col != 'b_flag']\n",
    "with open('AUDUSD_D1_2023buy/feature_names.json', 'w') as f:\n",
    "    json.dump(list(feature_names), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f65221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert index to datetime without 'unit' since the format is already date strings\n",
    "X_df.index = pd.to_datetime(X_df.index)\n",
    "\n",
    "# Format datetime to the desired string format\n",
    "X_df.index = X_df.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Creating a DataFrame for predictions with the correct index\n",
    "df_pred = pd.DataFrame(index=X_df.iloc[split:].index)  # No need for split+1\n",
    "df_pred['prediction'] = y_pred\n",
    "\n",
    "# Save to CSV\n",
    "df_pred.to_csv('predAUDUSD_D1_2023buy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece23c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
